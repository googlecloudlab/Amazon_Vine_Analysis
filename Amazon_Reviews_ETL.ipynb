{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BjjOTj1Wyls",
        "outputId": "c0e09bf8-2a8a-4f74-962b-185a2f43cd10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:2 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "spark_version = 'spark-3.3.0'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xKwTpATHqSe",
        "outputId": "273fb49d-8bd5-4ea2-e27a-0774f90a9b01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-10-03 04:04:51--  https://jdbc.postgresql.org/download/postgresql-42.2.16.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1002883 (979K) [application/java-archive]\n",
            "Saving to: ‘postgresql-42.2.16.jar’\n",
            "\n",
            "postgresql-42.2.16. 100%[===================>] 979.38K  4.98MB/s    in 0.2s    \n",
            "\n",
            "2022-10-03 04:04:52 (4.98 MB/s) - ‘postgresql-42.2.16.jar’ saved [1002883/1002883]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the Postgres driver that will allow Spark to interact with Postgres.\n",
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MMqDAjVS0KN9"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"M16-Amazon-Challenge\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.16.jar\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyBsySGuY-9V"
      },
      "source": [
        "### Load Amazon Data into Spark DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtCmBhQJY-9Z",
        "outputId": "47a028e8-1ef2-4fea-e013-97c92a72567c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-------------------+\n",
            "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|        review_date|\n",
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-------------------+\n",
            "|         US|   18446823|R35T75OLUGHL5C|B000NV6H94|     110804376|Stearns Youth Boa...|        Outdoors|          4|            0|          0|   N|                Y|          Four Stars|          GOOD VALUE|2015-08-31 00:00:00|\n",
            "|         US|   13724367|R2BV735O46BN33|B000IN0W3Y|     624096774|Primal Wear Men's...|        Outdoors|          5|            0|          0|   N|                Y|          Five Stars|  Excellent quality.|2015-08-31 00:00:00|\n",
            "|         US|   51001958|R2NBEUGPQQGXP1|B008RBJXFM|     278970944|Osprey Hydraulics...|        Outdoors|          4|            0|          0|   N|                Y|Only Flaw Is The Cap|3rd season using ...|2015-08-31 00:00:00|\n",
            "|         US|   32866903|R17LLAOJ8ITK0S|B00FK8WUQY|     312877650|CamelBak eddy .75...|        Outdoors|          3|            1|          1|   N|                Y|Poor design leads...|poor construction...|2015-08-31 00:00:00|\n",
            "|         US|   30907790|R39PEQBT5ISEF4|B00EZA3VW0|     305567912|Children Black Re...|        Outdoors|          1|            0|          0|   N|                Y|Very bad quality,...|Very bad quality,...|2015-08-31 00:00:00|\n",
            "|         US|   20232229|R3GNM3SU9VHJFT|B006JA8WEG|     842306035|Ibera Bicycle Tri...|        Outdoors|          4|            1|          1|   N|                Y|Nice bag. Should ...|Nice bag. Should ...|2015-08-31 00:00:00|\n",
            "|         US|   17698862| R2Y81OP0EK467|B002PWFSEO|     451480122|Therm-a-Rest Comp...|        Outdoors|          5|            0|          0|   N|                Y|Very comfortable ...|Gave this to my s...|2015-08-31 00:00:00|\n",
            "|         US|   38486114|R2LFGSI6HAYH5F|B002DZGKHW|     124386306|Sawyer Products P...|        Outdoors|          5|            1|          1|   N|                Y| Worked like a charm|Went on vacation ...|2015-08-31 00:00:00|\n",
            "|         US|   26319572|R297G6ED1IQO7W|B00ABA08F6|     991442421|Zippo Hand Warmer...|        Outdoors|          5|            1|          1|   N|                Y|Great item. Gets ...|Great item. Gets ...|2015-08-31 00:00:00|\n",
            "|         US|   27152337| RE27RFC6101N6|B003Z8WIHC|     886483892|Camp Chef Dutch O...|        Outdoors|          5|            0|          0|   N|                Y|Great value for t...|I am so glad I bo...|2015-08-31 00:00:00|\n",
            "|         US|   12516845|R3BPDME6E94W8Z|B007CP6UK0|     150224054|3CERA Portable Wi...|        Outdoors|          5|            0|          0|   N|                Y|          Five Stars|        good to have|2015-08-31 00:00:00|\n",
            "|         US|    3225242|R2P08O1RILUOX3|B003V3U9JK|     343847969|Texsport King Kot...|        Outdoors|          3|            0|          0|   N|                Y|Cot set up inconv...|VERY difficult to...|2015-08-31 00:00:00|\n",
            "|         US|     961839|R37CVAB03PTDVI|B00Y846HN8|     858088629|Wallygadgets 2 Wh...|        Outdoors|          5|            0|          1|   N|                Y|          Five Stars|Thanks excellent ...|2015-08-31 00:00:00|\n",
            "|         US|   47796452| RAWNWOGXPCPMD|B00IYQ84VY|     474493517|RainStoppers 34-I...|        Outdoors|          5|            0|          0|   N|                Y|          Five Stars|This umbrella is ...|2015-08-31 00:00:00|\n",
            "|         US|   32004835| R5DYGP6ASX77M|B002MYCKLY|     920014456|Alpha Deluxe Port...|        Outdoors|          5|            0|          0|   N|                Y|          Five Stars|Love it !! I even...|2015-08-31 00:00:00|\n",
            "|         US|   23972939|R1O0SAOOGF2KG7|B00EZV69JG|     128489321|Speedfil Z4 BTA B...|        Outdoors|          4|            0|          0|   N|                Y|        Good enough.|This is a fine mo...|2015-08-31 00:00:00|\n",
            "|         US|   40889047|R35NJUT0U3MU3V|B00AWOT3T8|     571303876|O'Brien Kids Plat...|        Outdoors|          5|            0|          0|   N|                Y| Got Up on First Try|We just bought th...|2015-08-31 00:00:00|\n",
            "|         US|   11244387|R242C08MF9D1AH|B0000AXTID|     739769424|Kwik-Tek F-5R Pla...|        Outdoors|          5|            0|          0|   N|                Y|They go over an a...|I have these on m...|2015-08-31 00:00:00|\n",
            "|         US|   20121211| R3RYG8TJTO4E2|B00IFHFJXI|     984009972|Ivation Portable ...|        Outdoors|          5|            0|          0|   N|                Y|Greatest Item I b...|This is the best ...|2015-08-31 00:00:00|\n",
            "|         US|   25657249|R3IKH1DNY0CP9F|B00KFILTWU|     405521681|GreenInsync Repla...|        Outdoors|          2|            0|          0|   N|                Y|I received this p...|I received this p...|2015-08-31 00:00:00|\n",
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkFiles\n",
        "url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Outdoors_v1_00.tsv.gz\"\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.option(\"encoding\", \"UTF-8\").csv(SparkFiles.get(\"amazon_reviews_us_Outdoors_v1_00.tsv.gz\"), sep=\"\\t\", header=True, inferSchema=True)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yUSe55VY-9t"
      },
      "source": [
        "### Create DataFrames to match tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8REmY1aY-9u",
        "outputId": "cfd2f5ea-4421-4a7d-df25-70879f9c6d5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-------------------+\n",
            "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|        review_date|\n",
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-------------------+\n",
            "|         US|   18446823|R35T75OLUGHL5C|B000NV6H94|     110804376|Stearns Youth Boa...|        Outdoors|          4|            0|          0|   N|                Y|          Four Stars|          GOOD VALUE|2015-08-31 00:00:00|\n",
            "|         US|   13724367|R2BV735O46BN33|B000IN0W3Y|     624096774|Primal Wear Men's...|        Outdoors|          5|            0|          0|   N|                Y|          Five Stars|  Excellent quality.|2015-08-31 00:00:00|\n",
            "|         US|   51001958|R2NBEUGPQQGXP1|B008RBJXFM|     278970944|Osprey Hydraulics...|        Outdoors|          4|            0|          0|   N|                Y|Only Flaw Is The Cap|3rd season using ...|2015-08-31 00:00:00|\n",
            "|         US|   32866903|R17LLAOJ8ITK0S|B00FK8WUQY|     312877650|CamelBak eddy .75...|        Outdoors|          3|            1|          1|   N|                Y|Poor design leads...|poor construction...|2015-08-31 00:00:00|\n",
            "|         US|   30907790|R39PEQBT5ISEF4|B00EZA3VW0|     305567912|Children Black Re...|        Outdoors|          1|            0|          0|   N|                Y|Very bad quality,...|Very bad quality,...|2015-08-31 00:00:00|\n",
            "|         US|   20232229|R3GNM3SU9VHJFT|B006JA8WEG|     842306035|Ibera Bicycle Tri...|        Outdoors|          4|            1|          1|   N|                Y|Nice bag. Should ...|Nice bag. Should ...|2015-08-31 00:00:00|\n",
            "|         US|   17698862| R2Y81OP0EK467|B002PWFSEO|     451480122|Therm-a-Rest Comp...|        Outdoors|          5|            0|          0|   N|                Y|Very comfortable ...|Gave this to my s...|2015-08-31 00:00:00|\n",
            "|         US|   38486114|R2LFGSI6HAYH5F|B002DZGKHW|     124386306|Sawyer Products P...|        Outdoors|          5|            1|          1|   N|                Y| Worked like a charm|Went on vacation ...|2015-08-31 00:00:00|\n",
            "|         US|   26319572|R297G6ED1IQO7W|B00ABA08F6|     991442421|Zippo Hand Warmer...|        Outdoors|          5|            1|          1|   N|                Y|Great item. Gets ...|Great item. Gets ...|2015-08-31 00:00:00|\n",
            "|         US|   27152337| RE27RFC6101N6|B003Z8WIHC|     886483892|Camp Chef Dutch O...|        Outdoors|          5|            0|          0|   N|                Y|Great value for t...|I am so glad I bo...|2015-08-31 00:00:00|\n",
            "|         US|   12516845|R3BPDME6E94W8Z|B007CP6UK0|     150224054|3CERA Portable Wi...|        Outdoors|          5|            0|          0|   N|                Y|          Five Stars|        good to have|2015-08-31 00:00:00|\n",
            "|         US|    3225242|R2P08O1RILUOX3|B003V3U9JK|     343847969|Texsport King Kot...|        Outdoors|          3|            0|          0|   N|                Y|Cot set up inconv...|VERY difficult to...|2015-08-31 00:00:00|\n",
            "|         US|     961839|R37CVAB03PTDVI|B00Y846HN8|     858088629|Wallygadgets 2 Wh...|        Outdoors|          5|            0|          1|   N|                Y|          Five Stars|Thanks excellent ...|2015-08-31 00:00:00|\n",
            "|         US|   47796452| RAWNWOGXPCPMD|B00IYQ84VY|     474493517|RainStoppers 34-I...|        Outdoors|          5|            0|          0|   N|                Y|          Five Stars|This umbrella is ...|2015-08-31 00:00:00|\n",
            "|         US|   32004835| R5DYGP6ASX77M|B002MYCKLY|     920014456|Alpha Deluxe Port...|        Outdoors|          5|            0|          0|   N|                Y|          Five Stars|Love it !! I even...|2015-08-31 00:00:00|\n",
            "|         US|   23972939|R1O0SAOOGF2KG7|B00EZV69JG|     128489321|Speedfil Z4 BTA B...|        Outdoors|          4|            0|          0|   N|                Y|        Good enough.|This is a fine mo...|2015-08-31 00:00:00|\n",
            "|         US|   40889047|R35NJUT0U3MU3V|B00AWOT3T8|     571303876|O'Brien Kids Plat...|        Outdoors|          5|            0|          0|   N|                Y| Got Up on First Try|We just bought th...|2015-08-31 00:00:00|\n",
            "|         US|   11244387|R242C08MF9D1AH|B0000AXTID|     739769424|Kwik-Tek F-5R Pla...|        Outdoors|          5|            0|          0|   N|                Y|They go over an a...|I have these on m...|2015-08-31 00:00:00|\n",
            "|         US|   20121211| R3RYG8TJTO4E2|B00IFHFJXI|     984009972|Ivation Portable ...|        Outdoors|          5|            0|          0|   N|                Y|Greatest Item I b...|This is the best ...|2015-08-31 00:00:00|\n",
            "|         US|   25657249|R3IKH1DNY0CP9F|B00KFILTWU|     405521681|GreenInsync Repla...|        Outdoors|          2|            0|          0|   N|                Y|I received this p...|I received this p...|2015-08-31 00:00:00|\n",
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import to_date,count\n",
        "# Read in the Review dataset as a DataFrame\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0TESUDRY-90",
        "outputId": "a44978e0-d044-44ca-e7b8-970ae5963a83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------------+\n",
            "|customer_id|customer_count|\n",
            "+-----------+--------------+\n",
            "|   43679767|             1|\n",
            "|   32024654|             1|\n",
            "|   52913169|             1|\n",
            "|   24297214|             1|\n",
            "|   26096454|             1|\n",
            "|   38247118|             1|\n",
            "|   44009906|             1|\n",
            "|    1753876|             1|\n",
            "|   26195644|             1|\n",
            "|   23042837|             1|\n",
            "|   11487525|             2|\n",
            "|   28258386|             1|\n",
            "|   42560427|             2|\n",
            "|   24540309|             1|\n",
            "|    1967239|             1|\n",
            "|   14217455|             1|\n",
            "|   20289484|            26|\n",
            "|   47103434|             1|\n",
            "|   28638887|             1|\n",
            "|     740678|             1|\n",
            "+-----------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create the customers_table DataFrame\n",
        "customers_df = df.groupby(\"customer_id\").agg(count(\"customer_id\")).withColumnRenamed(\"count(customer_id)\", \"customer_count\")\n",
        "customers_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FwXA6UvY-96",
        "outputId": "fa66621d-1a69-470d-ff00-c179f4327153"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+--------------------+\n",
            "|product_id|       product_title|\n",
            "+----------+--------------------+\n",
            "|B00IFHFJXI|Ivation Portable ...|\n",
            "|B00WG0J0D0|JanSport Superbre...|\n",
            "|B00V15AUN0|Nickelodeon Paw P...|\n",
            "|B00FUWSTI8|Bago Lightweight ...|\n",
            "|B003FV94NA|Michelin Lithion ...|\n",
            "|B00WIK04HO|Ultra Bright Camp...|\n",
            "|B00J2HSCM0|High Sierra Tank ...|\n",
            "|B009I6NSR4|Black Veil Brides...|\n",
            "|B001GSHSLE|Stansport 191 App...|\n",
            "|B00L2IO9M4|Columbia Sportswe...|\n",
            "|B00KY7IM7W|Nalgene 32 Oz Wid...|\n",
            "|B00TV5JCTK|Rollerblade ABEC ...|\n",
            "|B00B9D071Y|BUFF UV Multifunc...|\n",
            "|B00F9IGIKO|Condor Tactical F...|\n",
            "|B004X55L9I|Hydro Flask Insul...|\n",
            "|B00LORROIY|Scuba Choice Divi...|\n",
            "|B00AATRU8G|Kelty Redwing 44 ...|\n",
            "|B00HMCYWEO|Dakine Explorer L...|\n",
            "|B004DK1CM8|Hot Headz 12V Hea...|\n",
            "|B00T4W6SSS|Fits Sock Light H...|\n",
            "+----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create the products_table DataFrame and drop duplicates. \n",
        "products_df = df.select([\"product_id\",\"product_title\"]).drop_duplicates()\n",
        "products_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkqyCuNQY-9-",
        "outputId": "dfb3697e-8611-4dc4-e7f6-842d9c6c2168"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+-----------+----------+--------------+-----------+\n",
            "|     review_id|customer_id|product_id|product_parent|review_date|\n",
            "+--------------+-----------+----------+--------------+-----------+\n",
            "|R35T75OLUGHL5C|   18446823|B000NV6H94|     110804376| 2015-08-31|\n",
            "|R2BV735O46BN33|   13724367|B000IN0W3Y|     624096774| 2015-08-31|\n",
            "|R2NBEUGPQQGXP1|   51001958|B008RBJXFM|     278970944| 2015-08-31|\n",
            "|R17LLAOJ8ITK0S|   32866903|B00FK8WUQY|     312877650| 2015-08-31|\n",
            "|R39PEQBT5ISEF4|   30907790|B00EZA3VW0|     305567912| 2015-08-31|\n",
            "|R3GNM3SU9VHJFT|   20232229|B006JA8WEG|     842306035| 2015-08-31|\n",
            "| R2Y81OP0EK467|   17698862|B002PWFSEO|     451480122| 2015-08-31|\n",
            "|R2LFGSI6HAYH5F|   38486114|B002DZGKHW|     124386306| 2015-08-31|\n",
            "|R297G6ED1IQO7W|   26319572|B00ABA08F6|     991442421| 2015-08-31|\n",
            "| RE27RFC6101N6|   27152337|B003Z8WIHC|     886483892| 2015-08-31|\n",
            "|R3BPDME6E94W8Z|   12516845|B007CP6UK0|     150224054| 2015-08-31|\n",
            "|R2P08O1RILUOX3|    3225242|B003V3U9JK|     343847969| 2015-08-31|\n",
            "|R37CVAB03PTDVI|     961839|B00Y846HN8|     858088629| 2015-08-31|\n",
            "| RAWNWOGXPCPMD|   47796452|B00IYQ84VY|     474493517| 2015-08-31|\n",
            "| R5DYGP6ASX77M|   32004835|B002MYCKLY|     920014456| 2015-08-31|\n",
            "|R1O0SAOOGF2KG7|   23972939|B00EZV69JG|     128489321| 2015-08-31|\n",
            "|R35NJUT0U3MU3V|   40889047|B00AWOT3T8|     571303876| 2015-08-31|\n",
            "|R242C08MF9D1AH|   11244387|B0000AXTID|     739769424| 2015-08-31|\n",
            "| R3RYG8TJTO4E2|   20121211|B00IFHFJXI|     984009972| 2015-08-31|\n",
            "|R3IKH1DNY0CP9F|   25657249|B00KFILTWU|     405521681| 2015-08-31|\n",
            "+--------------+-----------+----------+--------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create the review_id_table DataFrame. \n",
        "# Convert the 'review_date' column to a date datatype with to_date(\"review_date\", 'yyyy-MM-dd').alias(\"review_date\")\n",
        "review_id_df = df.select([\"review_id\", \"customer_id\", \"product_id\", \"product_parent\", to_date(\"review_date\", 'yyyy-MM-dd').alias(\"review_date\")])\n",
        "review_id_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzMmkdKmY--D",
        "outputId": "0e805546-8329-4f4d-d80f-5250f58d999e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+-----------+-------------+-----------+----+-----------------+\n",
            "|     review_id|star_rating|helpful_votes|total_votes|vine|verified_purchase|\n",
            "+--------------+-----------+-------------+-----------+----+-----------------+\n",
            "|R35T75OLUGHL5C|          4|            0|          0|   N|                Y|\n",
            "|R2BV735O46BN33|          5|            0|          0|   N|                Y|\n",
            "|R2NBEUGPQQGXP1|          4|            0|          0|   N|                Y|\n",
            "|R17LLAOJ8ITK0S|          3|            1|          1|   N|                Y|\n",
            "|R39PEQBT5ISEF4|          1|            0|          0|   N|                Y|\n",
            "|R3GNM3SU9VHJFT|          4|            1|          1|   N|                Y|\n",
            "| R2Y81OP0EK467|          5|            0|          0|   N|                Y|\n",
            "|R2LFGSI6HAYH5F|          5|            1|          1|   N|                Y|\n",
            "|R297G6ED1IQO7W|          5|            1|          1|   N|                Y|\n",
            "| RE27RFC6101N6|          5|            0|          0|   N|                Y|\n",
            "|R3BPDME6E94W8Z|          5|            0|          0|   N|                Y|\n",
            "|R2P08O1RILUOX3|          3|            0|          0|   N|                Y|\n",
            "|R37CVAB03PTDVI|          5|            0|          1|   N|                Y|\n",
            "| RAWNWOGXPCPMD|          5|            0|          0|   N|                Y|\n",
            "| R5DYGP6ASX77M|          5|            0|          0|   N|                Y|\n",
            "|R1O0SAOOGF2KG7|          4|            0|          0|   N|                Y|\n",
            "|R35NJUT0U3MU3V|          5|            0|          0|   N|                Y|\n",
            "|R242C08MF9D1AH|          5|            0|          0|   N|                Y|\n",
            "| R3RYG8TJTO4E2|          5|            0|          0|   N|                Y|\n",
            "|R3IKH1DNY0CP9F|          2|            0|          0|   N|                Y|\n",
            "+--------------+-----------+-------------+-----------+----+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create the vine_table. DataFrame\n",
        "vine_df = df.select([\"review_id\", \"star_rating\", \"helpful_votes\", \"total_votes\", \"vine\", \"verified_purchase\"])\n",
        "vine_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jITZhLkmY--J"
      },
      "source": [
        "### Connect to the AWS RDS instance and write each DataFrame to its table. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jiUvs1aY--L",
        "outputId": "0791cbb4-e52c-414c-a70d-6022c2d4dd02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter database password··········\n"
          ]
        }
      ],
      "source": [
        "# Store environment variable\n",
        "from getpass import getpass\n",
        "password = getpass('Enter database password')\n",
        "\n",
        "# Configure settings for RDS\n",
        "mode = \"append\"\n",
        "jdbc_url=\"<instance>:5432/<database>\"\n",
        "config = {\"user\":\"postgres\", \n",
        "          \"password\": password, \n",
        "          \"driver\":\"org.postgresql.Driver\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "T2zgZ-aKY--Q"
      },
      "outputs": [],
      "source": [
        "# Write review_id_df to table in RDS\n",
        "review_id_df.write.jdbc(url=jdbc_url, table='review_id_table', mode=mode, properties=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1m3yzn-LY--U",
        "outputId": "39ed14c4-e8a6-429a-b055-0ad59e987a09"
      },
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-f9bb330a0bb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Write products_df to table in RDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# about 3 min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mproducts_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjdbc_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'products_table'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.3.0-bin-hadoop3/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m             \u001b[0mjprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.0-bin-hadoop3/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o166.jdbc.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 30.0 failed 1 times, most recent failure: Lost task 1.0 in stage 30.0 (TID 28) (22a22a40fba5 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO products_table (\"product_id\",\"product_title\") VALUES ('B00QESU5QQ','Yukiss® Mini Solar Lantern. Hanging, Portable Rainproof, Garden Decorative, Rechargeable, Indoor and Outdoor (!), Solar Powered LED Light. Bulb Has High Home Output (72 Lumens). Charge By Normal Sunlight or USB (Usb Charging Cable Included)') was aborted: ERROR: duplicate key value violates unique constraint \"products_table_pkey\"\n  Detail: Key (product_id)=(B00QESU5QQ) already exists.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:169)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2286)\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1404)\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1429)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:507)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:870)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:893)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1639)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:708)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:868)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:867)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"products_table_pkey\"\n  Detail: Key (product_id)=(B00QESU5QQ) already exists.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2553)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2285)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:867)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:757)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO products_table (\"product_id\",\"product_title\") VALUES ('B00QESU5QQ','Yukiss® Mini Solar Lantern. Hanging, Portable Rainproof, Garden Decorative, Rechargeable, Indoor and Outdoor (!), Solar Powered LED Light. Bulb Has High Home Output (72 Lumens). Charge By Normal Sunlight or USB (Usb Charging Cable Included)') was aborted: ERROR: duplicate key value violates unique constraint \"products_table_pkey\"\n  Detail: Key (product_id)=(B00QESU5QQ) already exists.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:169)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2286)\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1404)\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1429)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:507)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:870)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:893)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1639)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:708)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:868)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:867)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"products_table_pkey\"\n  Detail: Key (product_id)=(B00QESU5QQ) already exists.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2553)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2285)\n\t... 20 more\n"
          ]
        }
      ],
      "source": [
        "# Write products_df to table in RDS\n",
        "# about 3 min\n",
        "products_df.write.jdbc(url=jdbc_url, table='products_table', mode=mode, properties=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "KbXri15fY--Z"
      },
      "outputs": [],
      "source": [
        "# Write customers_df to table in RDS\n",
        "# 5 min 14 s\n",
        "customers_df.write.jdbc(url=jdbc_url, table='customers_table', mode=mode, properties=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "XdQknSHLY--e"
      },
      "outputs": [],
      "source": [
        "# Write vine_df to table in RDS\n",
        "# 11 minutes\n",
        "vine_df.write.jdbc(url=jdbc_url, table='vine_table', mode=mode, properties=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Exuo6ebUsCqW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.2 64-bit ('3.9.2')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.2"
    },
    "nteract": {
      "version": "0.12.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "a3406f8e3981ff008e2601c646d1edcb3854e24105c672c5fed5c725a0654e60"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
